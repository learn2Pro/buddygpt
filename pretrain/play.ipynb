{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f391e49f-c01a-4d48-a437-55b9d146df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TinyllmRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        \"\"\" 旋转位置编码\n",
    "            - dim (int): 旋转嵌入的维度大小。\n",
    "            - max_position_embeddings (int): 预计算的最大位置嵌入数，默认为2048。\n",
    "            - base (int): 用于计算逆频率的基本频率，默认为10000。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        # 计算逆频率值，并将其注册为模型的缓冲区\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # 为了支持`torch.jit.trace`功能，立即计算预存储的余弦和正弦缓存\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        \"\"\" 预计算的余弦和正弦缓存\n",
    "        \"\"\"\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        # 创建一个从0到最大序列长度-1的整数张量，与 inv_freq 具有相同的设备和数据类型\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
    "\n",
    "        # 计算每个位置与每个维度的频率，形成频谱矩阵\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        \n",
    "        # 不同于论文中的实现，这里采用了不同的排列方式以获得相同的计算结果\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\" 旋转输入一半的 hidden dim\n",
    "    \"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.mistral.modeling_mistral.apply_rotary_pos_emb\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\" 在 qk 应用旋转位置编码\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): q\n",
    "        k (`torch.Tensor`): k\n",
    "        cos (`torch.Tensor`): 旋转位置嵌入的余弦部分\n",
    "        sin (`torch.Tensor`): 旋转位置嵌入的正弦部分\n",
    "        position_ids (`torch.Tensor`): 与q和k对应位置的标记索引。例如，在处理KV缓存时，可以使用偏移过的位置ID。\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1): 'unsqueeze_dim' 参数指定了沿哪个维度对 cos[position_ids] \n",
    "            和 sin[position_ids] 进行扩展，以便它们能够适当地广播到 q 和 k 的维度上。\n",
    "            例如，注意 cos[position_ids] 和 sin[position_ids] 具有形状 [batch_size, seq_len, head_dim]。\n",
    "            那么，如果 q 和 k 的形状分别为 [batch_size, heads, seq_len, head_dim]，\n",
    "            则设置 unsqueeze_dim=1 可使 cos[position_ids] 和 sin[position_ids] 可以广播到 q 和 k 的形状上。\n",
    "            同样地，如果 q 和 k 的形状为 [batch_size, seq_len, heads, head_dim]，则应将 unsqueeze_dim 设置为 2\n",
    "    Returns:\n",
    "        包含使用旋转位置嵌入变换后的q和k张量的 `tuple(torch.Tensor)`。\n",
    "    \"\"\"\n",
    "    # print(\"ori cos: \", cos.shape)\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # print(\"q: \", q.shape)\n",
    "    # print(\"cos: \", cos.shape)\n",
    "    # print(\"sin: \", sin.shape)\n",
    "    # print(\"rotate_half: \", rotate_half(q).shape)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1a7c8ef-b766-45fd-ba73-060b6003674e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 12, 16]), torch.Size([4, 4, 6, 16]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k = torch.randn(4, 4, 12, 16), torch.randn(4, 4, 6, 16) # (bs, n_head, seq_len, n_dim)\n",
    "rotary_emb = TinyllmRotaryEmbedding(dim=16)\n",
    "cos, sin = rotary_emb(q, seq_len=4)\n",
    "q, k = apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=2)\n",
    "q.shape, k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e2922a-0f09-45cc-91d3-9746eb9349b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 12, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(12, 16)[None].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0391cb16-3cb6-4283-849d-963ba1ec4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c381d-b936-4498-84f4-7cbf17b1cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain import load_model_tokenizer\n",
    "\n",
    "\n",
    "tokenizer, model = load_model_tokenizer(tokenizer_name='Qwen/Qwen3-0.6B', seq_len=1024, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1cf224b-4d27-48f2-a6c8-a840d5639e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'中国首都是哪?但如果但如果但如果voltvoltvoltvoltvoltvolt但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果但如果'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pretrain import sample\n",
    "sample(tokenizer, model, '中国首都是哪?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c9d5d-24b8-4d53-a7ee-3350ec680072",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(tokenizer, num_proc=args.ds_num_proc, seq_len=args.block_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
