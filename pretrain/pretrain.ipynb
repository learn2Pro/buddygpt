{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gpt2-llama2-llama3](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/gpt2-to-llama2-llama3.webp?1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/llama3-to-llama31.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from configuration_buddygpt import BuddyGPTConfig\n",
    "from modeling_buddygpt import BuddyGPTForCausalLM\n",
    "\n",
    "\n",
    "output_dir = f'outputs/buddygpt-qwen3'\n",
    "block_size = 1024\n",
    "# uer/gpt2-chinese-cluecorpussmall\n",
    "# Qwen/Qwen3-0.6B\n",
    "# gpt2\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-0.6B' ,trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "config = BuddyGPTConfig(\n",
    "        hidden_size=1024,\n",
    "        num_hidden_layers=24,\n",
    "        num_attention_heads=16,\n",
    "        num_key_value_heads=8,\n",
    "        intermediate_size=2048,\n",
    "        rope_theta=10000.0,\n",
    "        num_seq_len=block_size,\n",
    "        vocab_size=len(tokenizer),\n",
    "        tie_word_embeddings=True,\n",
    "    ) \n",
    "model = BuddyGPTForCausalLM(config)\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(model)\n",
    "# print(sum([p.numel() for p in model.parameters()]) / 1024 / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_parameters(model):\n",
    "    num_param = sum([param.numel() for param in model.parameters() if param.requires_grad])\n",
    "    print(f'total param {num_param/1024/1024}m')\n",
    "    \n",
    "def sample(model, query, max_length=50):\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_length,\n",
    "    )\n",
    "    gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return gen_text\n",
    "\n",
    "model.to(device)\n",
    "print_parameters(model)\n",
    "sample(model, '中国首都是哪?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "# 50m model need 20*50m = 1B token\n",
    "# 100m model need 20*100m = 2B token\n",
    "# 200m model need 20*200m = 4B token\n",
    "# 500m model need 20*500m = 10B token\n",
    "\n",
    "# Total tokens: 1872137976\n",
    "# 1.8B token\n",
    "ds = load_dataset(\"pleisto/wikipedia-cn-20230720-filtered\", split=\"train\")\n",
    "# zhihu instruction\n",
    "zhihu_ds = load_dataset(\"wangrui6/Zhihu-KOL\", split=\"train\")\n",
    "# 10B token\n",
    "web_ds = load_dataset(\"HuggingFaceFW/fineweb\", \"sample-10BT\", split=\"train\")\n",
    "# firefly ds\n",
    "# 13B token\n",
    "ff_ds = load_dataset(\"YeungNLP/firefly-pretrain-dataset\", split=\"train\")\n",
    "# novel ds\n",
    "novel_ds = load_dataset(\"wdndev/webnovel-chinese\", split=\"train\")\n",
    "\n",
    "# 拼接并切块\n",
    "def group_texts_with_padding(examples):\n",
    "    # block_size = block_size\n",
    "    concatenated = sum(examples[\"input_ids\"], [])\n",
    "    # result = {\"input_ids\": []}\n",
    "    total_length = len(concatenated)\n",
    "    result = {\n",
    "        \"input_ids\": [concatenated[i:i+block_size] for i in range(0, total_length, block_size)]\n",
    "    }\n",
    "    return result\n",
    "    \n",
    "def encode(examples, field: str = 'text'):\n",
    "    result = tokenizer(examples[field])\n",
    "    return result\n",
    "\n",
    "def encode_instruction(examples):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for i in range(len(examples['INSTRUCTION'])):\n",
    "        instruction = examples['INSTRUCTION'][i]\n",
    "        response = examples['RESPONSE'][i]\n",
    "        build_instruction = f\"### Instruction:\\n{instruction}\\n### Response:\\n{response}\"\n",
    "        tokenized_instruction = tokenizer(build_instruction)\n",
    "        input_ids.append(tokenized_instruction['input_ids'])\n",
    "        attention_mask.append(tokenized_instruction['attention_mask'])\n",
    "    result = {}\n",
    "    result['input_ids'] = input_ids\n",
    "    result['attention_mask'] = attention_mask\n",
    "    return result\n",
    "\n",
    "ds = ds.map(lambda x: encode(x, 'completion'), batched=True, num_proc=30, remove_columns=ds.column_names)\n",
    "ds = ds.map(group_texts_with_padding, batched=True, num_proc=30, remove_columns=ds.column_names)\n",
    "zhihu_ds = zhihu_ds.map(encode_instruction, batched=True, num_proc=30, remove_columns=zhihu_ds.column_names)\n",
    "zhihu_ds = zhihu_ds.map(group_texts_with_padding, batched=True, num_proc=30, remove_columns=zhihu_ds.column_names)\n",
    "ff_ds = ff_ds.map(encode, batched=True, num_proc=30, remove_columns=ff_ds.column_names)\n",
    "ff_ds = ff_ds.map(group_texts_with_padding, batched=True, num_proc=30, remove_columns=ff_ds.column_names)\n",
    "novel_ds = novel_ds.map(encode, batched=True, num_proc=30, remove_columns=novel_ds.column_names)\n",
    "novel_ds = novel_ds.map(group_texts_with_padding, batched=True, num_proc=30, remove_columns=novel_ds.column_names)\n",
    "# web_ds = web_ds.map(encode, batched=True, num_proc=30, remove_columns=web_ds.column_names)\n",
    "# web_ds = web_ds.map(group_texts_with_padding, batched=True, num_proc=30, remove_columns=web_ds.column_names)\n",
    "ds = concatenate_datasets([ds, ff_ds, novel_ds, zhihu_ds])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, TrainerCallback, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "from datetime import datetime\n",
    "\n",
    "# TF32 设置（建议启用）\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# print(sample(model, '中国首都是哪?'))\n",
    "# buddygpt.FLASH = 1\n",
    "now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "class SampleTextCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.global_step % 100 == 0:\n",
    "            prompt = \"中国首都是哪?\"\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=128,\n",
    "            )\n",
    "            gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Sample generated at step {state.global_step}]:\\n{gen_text}\\n\")\n",
    "        \n",
    "        if state.global_step % 100 == 0:\n",
    "            prompt = \"which is the capital of china?\"\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=128,\n",
    "            )\n",
    "            gen_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Sample generated at step {state.global_step}]:\\n{gen_text}\\n\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "# 创建 collator\n",
    "# data_collator = DataCollatorForSeq2Seq(\n",
    "#     tokenizer=tokenizer,\n",
    "#     model=model,\n",
    "#     label_pad_token_id=-100,  # 默认是 -100，loss 不计算这个 token\n",
    "#     padding=True\n",
    "# )\n",
    "\n",
    "# TL;DR\n",
    "# Action\tWhy\n",
    "# ✅ max_grad_norm=1.0\tClip exploding gradients\n",
    "# ✅ Lower learning_rate\tReduce gradient magnitude\n",
    "# ✅ Increase warmup_steps\tStabilize early training\n",
    "# ✅ Use gradient_accumulation_steps\tSmooth out spikes\n",
    "# ✅ Monitor layers with high grad norm\tFind root cause\n",
    "\n",
    "args = TrainingArguments(\n",
    "    run_name=f'buddygpt-{now}',\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.2,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=10,\n",
    "    bf16=True,\n",
    "    # fp16=True,\n",
    "    # max_steps=1,\n",
    "    # remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=128,\n",
    "    # eval_strategy=\"steps\",  # or eval_strategy=\"steps\" in newer versions\n",
    "    # eval_steps=500,              # Correct parameter name\n",
    "    save_safetensors=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds,\n",
    "    callbacks=[SampleTextCallback],\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
