{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d691bfe-ba66-4d59-a139-fd34f2a7f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Make sure project root is in the path\n",
    "\n",
    "from sft import do_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b58947e-49fb-4b30-a48b-b7511324fb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151669\n",
      "BuddyGPTForCausalLM(\n",
      "  (model): BuddyGPTModel(\n",
      "    (embed_tokens): Embedding(151669, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x DecoderLayer(\n",
      "        (self_attn): SdpaAttention(\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GateMLP(\n",
      "          (gate_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (post_layernorm): RMSNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=151669, bias=False)\n",
      ")\n",
      "total param 151.60986328125m\n"
     ]
    }
   ],
   "source": [
    "from sft import load_tokenizer_model, load_dataset\n",
    "\n",
    "device = 'cuda'\n",
    "tokenizer, model = load_tokenizer_model('../outputs/buddygpt-0.1b-base', 1024, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877e28a9-188b-4565-9621-b4c0f1ec6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b70630f9aa4323a5e5c012295610b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=30):   0%|          | 0/1649399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1649399\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['<|im_start|>user\\n自然语言推理：\\n前提：家里人心甘情愿地养他,还有几家想让他做女婿的\\n假设：他是被家里人收养的孤儿<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n中立<|im_end|>\\n',\n",
       "  '<|im_start|>user\\n在上海的苹果代工厂，较低的基本工资让工人们形成了“软强制”的加班默契。加班能多拿两三千，“自愿”加班成为常态。律师提示，加班后虽能获得一时不错的报酬，但过重的工作负荷会透支身体，可能对今后劳动权利造成不利影响。\\n输出摘要：<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n苹果代工厂员工调查：为何争着“自愿”加班<|im_end|>\\n']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(tokenizer, 30, 1024)\n",
    "ds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e3e828-5567-44bf-9e12-85df6297c081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "生成音乐热评<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "1、感謝大家支持！然後就是Jane的人聲真的太好聽了，remix很自然就做出來了哈哈。為了讓人聲情感突出，這版本並沒有用太多搶耳的音色，只保留了一點點自己bass風格的律動感，然後我拿起一把Fender吉他就已經是主旋律了。Anyway , 超榮幸能參與這張專輯的！ Enjoy！\n",
      "2、这首\n",
      "有种退一步现世安稳，进一步天地广阔的感觉\n",
      "好适合拿去当剪辑的BGM\n",
      "在成长逐渐变得温柔又坚强的那种<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds[3]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69b69d68-bbd7-414a-bfff-9b56d92f19db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     11,\n",
       "           3465,    553,   3960,     17,    776,      0, 151645,    198, 151644,\n",
       "            872,    198,  44063,  87752,  99534,  16744, 101047,  55338,   2073,\n",
       "            285,    854,  27733,  99689, 109762,   2073,  16123,    854,   8997,\n",
       "            785,  19145,    374,  17923,    323,    279,   4349,    374,   2238,\n",
       "             13,    576,   9853,  12644,    374,  49584,    304,    279,   7015,\n",
       "            323,    279,  29464,   1021,    374,  35918,     13, 151645,    198,\n",
       "         151644,  77091,    198, 151667,    271, 151668,    271,    785,  19145,\n",
       "            572,  17923,    323,    279,   4349,    572,   2238,     13,    576,\n",
       "           9853,  12644,    572,  49584,    304,    279,   7015,    323,    279,\n",
       "          29464,   1021,    572,  35918,     13, 151645,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 3. Tokenize 文本（重点）\n",
    "encoded = tokenizer(ds[:1]['text'], return_tensors=\"pt\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84965fbd-eaea-48e7-9af0-4b7f91732f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14990, 151645]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hello<|im_end|>', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd74e13f-68d4-400d-969c-d38450c8c9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     11,\n",
       "           3465,    553,   3960,     17,    776,      0, 151645,    198, 151644,\n",
       "            872,    198,  44063,  87752,  99534,  16744, 101047,  55338,   2073,\n",
       "            285,    854,  27733,  99689, 109762,   2073,  16123,    854,   8997,\n",
       "            785,  19145,    374,  17923,    323,    279,   4349,    374,   2238,\n",
       "             13,    576,   9853,  12644,    374,  49584,    304,    279,   7015,\n",
       "            323,    279,  29464,   1021,    374,  35918,     13, 151645,    198,\n",
       "         151644,  77091,    198, 151667,    271, 151668,    271,    785,  19145,\n",
       "            572,  17923,    323,    279,   4349,    572,   2238,     13,    576,\n",
       "           9853,  12644,    572,  49584,    304,    279,   7015,    323,    279,\n",
       "          29464,   1021,    572,  35918,     13, 151645,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,    198, 151667,    271, 151668,    271,    785,  19145,\n",
       "            572,  17923,    323,    279,   4349,    572,   2238,     13,    576,\n",
       "           9853,  12644,    572,  49584,    304,    279,   7015,    323,    279,\n",
       "          29464,   1021,    572,  35918,     13, 151645,    198]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "# text = '<|im_start|>system\\nYou are a helpful assistant, created by learn2pro!<|im_end|>\\n<|im_start|>user\\n将以下短文中的所有“is”动词改为“was”。\\nThe cake is delicious and the pie is too. The ice cream is melting in the sun and the lemonade is refreshing.<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nThe cake was delicious and the pie was too. The ice cream was melting in the sun and the lemonade was refreshing.<|im_end|>\\n'\n",
    "\n",
    "# sample = {\n",
    "#     \"text\": \"<|im_start|>user\\n中国的首都是哪？<|im_end|>\\n<|im_start|>assistant\\n中国的首都是北京。<|im_end|>\"\n",
    "# }\n",
    "sample = {\n",
    "    \"input_ids\": [151644,   8948,    198,   2610,    525,    264,  10950,  17847,     11,\n",
    "           3465,    553,   3960,     17,    776,      0, 151645,    198, 151644,\n",
    "            872,    198,  44063,  87752,  99534,  16744, 101047,  55338,   2073,\n",
    "            285,    854,  27733,  99689, 109762,   2073,  16123,    854,   8997,\n",
    "            785,  19145,    374,  17923,    323,    279,   4349,    374,   2238,\n",
    "             13,    576,   9853,  12644,    374,  49584,    304,    279,   7015,\n",
    "            323,    279,  29464,   1021,    374,  35918,     13, 151645,    198,\n",
    "         151644,  77091,    198, 151667,    271, 151668,    271,    785,  19145,\n",
    "            572,  17923,    323,    279,   4349,    572,   2238,     13,    576,\n",
    "           9853,  12644,    572,  49584,    304,    279,   7015,    323,    279,\n",
    "          29464,   1021,    572,  35918,     13, 151645,    198]\n",
    "}\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer=tokenizer,\n",
    "    instruction_template=\"<|im_start|>user\",  # 开始 loss 的位置\n",
    "    response_template=\"<|im_start|>assistant\",  # 如果你想从 assistant 开始一直算 loss，可以省略\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "batch = collator([sample])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ddccd14-7877-4154-994b-b122c4cc258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> | Label: -100\n",
      "system     | Label: -100\n",
      "\n",
      "          | Label: -100\n",
      "You        | Label: -100\n",
      " are       | Label: -100\n",
      " a         | Label: -100\n",
      " helpful   | Label: -100\n",
      " assistant | Label: -100\n",
      ",          | Label: -100\n",
      " created   | Label: -100\n",
      " by        | Label: -100\n",
      " learn     | Label: -100\n",
      "2          | Label: -100\n",
      "pro        | Label: -100\n",
      "!          | Label: -100\n",
      "<|im_end|> | Label: -100\n",
      "\n",
      "          | Label: -100\n",
      "<|im_start|> | Label: -100\n",
      "user       | Label: -100\n",
      "\n",
      "          | Label: -100\n",
      "将          | Label: -100\n",
      "以下         | Label: -100\n",
      "短          | Label: -100\n",
      "文          | Label: -100\n",
      "中的         | Label: -100\n",
      "所有         | Label: -100\n",
      "“          | Label: -100\n",
      "is         | Label: -100\n",
      "”          | Label: -100\n",
      "动          | Label: -100\n",
      "词          | Label: -100\n",
      "改为         | Label: -100\n",
      "“          | Label: -100\n",
      "was        | Label: -100\n",
      "”          | Label: -100\n",
      "。\n",
      "         | Label: -100\n",
      "The        | Label: -100\n",
      " cake      | Label: -100\n",
      " is        | Label: -100\n",
      " delicious | Label: -100\n",
      " and       | Label: -100\n",
      " the       | Label: -100\n",
      " pie       | Label: -100\n",
      " is        | Label: -100\n",
      " too       | Label: -100\n",
      ".          | Label: -100\n",
      " The       | Label: -100\n",
      " ice       | Label: -100\n",
      " cream     | Label: -100\n",
      " is        | Label: -100\n",
      " melting   | Label: -100\n",
      " in        | Label: -100\n",
      " the       | Label: -100\n",
      " sun       | Label: -100\n",
      " and       | Label: -100\n",
      " the       | Label: -100\n",
      " lemon     | Label: -100\n",
      "ade        | Label: -100\n",
      " is        | Label: -100\n",
      " refreshing | Label: -100\n",
      ".          | Label: -100\n",
      "<|im_end|> | Label: -100\n",
      "\n",
      "          | Label: -100\n",
      "<|im_start|> | Label: -100\n",
      "assistant  | Label: -100\n",
      "\n",
      "          | Label: 198\n",
      "<think>    | Label: 151667\n",
      "\n",
      "\n",
      "         | Label: 271\n",
      "</think>   | Label: 151668\n",
      "\n",
      "\n",
      "         | Label: 271\n",
      "The        | Label: 785\n",
      " cake      | Label: 19145\n",
      " was       | Label: 572\n",
      " delicious | Label: 17923\n",
      " and       | Label: 323\n",
      " the       | Label: 279\n",
      " pie       | Label: 4349\n",
      " was       | Label: 572\n",
      " too       | Label: 2238\n",
      ".          | Label: 13\n",
      " The       | Label: 576\n",
      " ice       | Label: 9853\n",
      " cream     | Label: 12644\n",
      " was       | Label: 572\n",
      " melting   | Label: 49584\n",
      " in        | Label: 304\n",
      " the       | Label: 279\n",
      " sun       | Label: 7015\n",
      " and       | Label: 323\n",
      " the       | Label: 279\n",
      " lemon     | Label: 29464\n",
      "ade        | Label: 1021\n",
      " was       | Label: 572\n",
      " refreshing | Label: 35918\n",
      ".          | Label: 13\n",
      "<|im_end|> | Label: 151645\n",
      "\n",
      "          | Label: 198\n"
     ]
    }
   ],
   "source": [
    "for input_id, label in zip(batch['input_ids'][0], batch['labels'][0]):\n",
    "    token = tokenizer.decode([input_id])\n",
    "    print(f\"{token:<10} | Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8339892-eb67-44dd-b4ba-a6818df68535",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = do_sample(tokenizer, model, '中国首都是哪?')\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1472c-b37c-4760-a97b-056ab7c95385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from model.utils import calc_token\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import model.modeling_tinyllm\n",
    "\n",
    "model_id = 'learn2pro/buddygpt-0.2b-chat-zh'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "\n",
    "# ds = load_dataset(\"wangrui6/Zhihu-KOL\", split=\"train\")\n",
    "ds = load_dataset(\"openbmb/Ultra-FineWeb\", split=\"zh\")\n",
    "calc_token(ds, tokenizer, field='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bfa01b-e822-4e40-86b7-9ce2fa021805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
